{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config.config import METRICS, THRESHOLDS\n",
    "from utils.io import load_validated_data, export_eval_data, plot_metrics, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = [\"STREET\", \"FULLNAME\", \"BOTH\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    if dataset_name == \"BOTH\":\n",
    "        data_street = load_validated_data(\"STREET\")\n",
    "        data_fullname = load_validated_data(\"FULLNAME\")\n",
    "        DATA = pd.concat([data_street, data_fullname], ignore_index=True)\n",
    "    else: \n",
    "        DATA = load_validated_data(dataset_name)\n",
    "    \n",
    "    if DATA.empty:\n",
    "        print(f\"No data found for dataset '{dataset_name}'. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    eval_df = pd.DataFrame(index=METRICS)\n",
    "    for algo in THRESHOLDS:\n",
    "        y_true_col = f\"{algo.upper()}_TRUE_MATCH\"\n",
    "        y_pred_col = f\"{algo.upper()}_BEST_MATCH_BINARY\"\n",
    "        \n",
    "        if y_true_col not in DATA.columns or y_pred_col not in DATA.columns:\n",
    "            print(f\"Columns for algorithm '{algo}' not found in dataset '{dataset_name}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        y_true = DATA[y_true_col]\n",
    "        y_pred = DATA[y_pred_col]\n",
    "        \n",
    "        mask = y_true.notna() & y_pred.notna()\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        \n",
    "        if y_true.empty:\n",
    "            print(f\"No valid data for algorithm '{algo}' in dataset '{dataset_name}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        if y_true.dtype == \"object\":\n",
    "            y_true = y_true.astype(bool)\n",
    "            y_pred = y_pred.astype(bool)\n",
    "        \n",
    "        valid_labels = (0, 1)\n",
    "        if not set(y_true.unique()).issubset(valid_labels) or not set(y_pred.unique()).issubset(valid_labels):\n",
    "            print(f\"Invalid labels detected in algorithm '{algo}' for dataset '{dataset_name}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        eval_df[algo] = [accuracy, precision, recall, f1, roc_auc]\n",
    "    \n",
    "    if eval_df.empty:\n",
    "        print(f\"No evaluation data for dataset '{dataset_name}'. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    eval_df = export_eval_data(eval_df, dataset_name)\n",
    "    metrics_fig = plot_metrics(eval_df,  dataset_name)\n",
    "    plt.close(metrics_fig)\n",
    "    \n",
    "    cm_list = []\n",
    "    for algo in THRESHOLDS:\n",
    "        y_true_col = f\"{algo.upper()}_TRUE_MATCH\"\n",
    "        y_pred_col = f\"{algo.upper()}_BEST_MATCH_BINARY\"\n",
    "        \n",
    "        if y_true_col not in DATA.columns or y_pred_col not in DATA.columns:\n",
    "            continue\n",
    "\n",
    "        y_true = DATA[y_true_col]\n",
    "        y_pred = DATA[y_pred_col]\n",
    "\n",
    "        mask = y_true.notna() & y_pred.notna()\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        \n",
    "        if y_true.empty:\n",
    "            continue\n",
    "            \n",
    "        if y_true.dtype == \"object\":\n",
    "            y_true = y_true.astype(bool)\n",
    "            y_pred = y_pred.astype(bool)\n",
    "        \n",
    "        valid_labels = (0, 1)\n",
    "        if not set(y_true.unique()).issubset(valid_labels) or not set(y_pred.unique()).issubset(valid_labels):\n",
    "            continue\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_fig = plot_confusion_matrix(cm, algo, dataset_name)\n",
    "        cm_list.append(cm_fig)\n",
    "        plt.close(cm_fig)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
